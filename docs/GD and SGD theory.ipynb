{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Convergence and Stability of Gradient Descent for Linear Regression Problems"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (I) Gradient Descent "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Consider the following problem:\r\n",
    "\r\n",
    "$\\min_{\\alpha,\\beta} \\hat{Q}(\\alpha,\\beta) \\equiv \\min_{\\alpha,\\beta} \\frac{1}{N} \\sum_{i=1}^N \\big(y_i - \\alpha - \\beta x_i\\big)^2 $"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The gradinet of $\\hat{Q}$ can be written as :\n",
    "\n",
    "$\\nabla \\hat{Q}(\\alpha,\\beta) \\equiv  \\begin{pmatrix} \\frac{\\partial \\hat{Q}(\\alpha,\\beta)}{\\partial \\alpha}\\\\ \\frac{\\partial \\hat{Q}(\\alpha,\\beta)}{\\partial \\alpha} \\end{pmatrix}=    -\\frac{2}{N} \\begin{pmatrix} \\sum_{i=1}^N  (y_i - \\beta x_i - \\alpha)\\\\  \\sum_{i=1}^N  (y_i - \\beta x_i - \\alpha)x_i \\end{pmatrix}$\n",
    "\n",
    "which can be rewritten as:\n",
    "\n",
    "$\\nabla \\hat{Q}(\\alpha,\\beta) =  -2 \\begin{pmatrix}  \\bar{y} - \\beta \\bar{x} - \\alpha\\\\  \\bar{yx} - \\beta \\bar{x^2} - \\alpha \\bar{x} \\end{pmatrix}$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now consider a simple gradient descent (GD) algorith that works as follows\n",
    "\n",
    "\n",
    "$\\begin{pmatrix} \\alpha_{t+1}\\\\ \\beta_{t+1} \\end{pmatrix} = \\begin{pmatrix} \\alpha_{t}\\\\ \\beta_{t} \\end{pmatrix} - \\lambda \\nabla \\hat{Q}(\\alpha_t,\\beta_t) $\n",
    "\n",
    "where $t$ is the t-$th$ step of the optimization, $\\lambda$ is the step size in the opposite direction of the gradient, and $(\\alpha_0,\\beta_0)$ is the initialization in the optimization algorithm. Exapanding the above formula leads to \n",
    "\n",
    "$\\begin{pmatrix} \\alpha_{t+1}\\\\ \\beta_{t+1} \\end{pmatrix} = \\begin{bmatrix}1 - 2\\lambda & -2 \\lambda \\bar{x}\\\\ - 2 \\lambda \\bar{x} & 1-2\\lambda \\bar{x^2}\\end{bmatrix} \\begin{pmatrix} \\alpha_{t}\\\\ \\beta_{t} \\end{pmatrix} + \\begin{pmatrix} 2 \\lambda \\bar{y}\\\\ 2 \\lambda \\bar{xy} \\end{pmatrix}$\n",
    "\n",
    "Defining $\\vec{s}_t \\equiv \\begin{pmatrix} \\alpha_{t}\\\\ \\beta_{t} \\end{pmatrix} $\n",
    "\n",
    "$A \\equiv \\begin{bmatrix}1 - 2\\lambda & -2 \\lambda \\bar{x}\\\\ - 2 \\lambda \\bar{x} & 1-2\\lambda \\bar{x^2}\\end{bmatrix} $ and  $B \\equiv \\begin{pmatrix} 2\\lambda \\bar{y}\\\\ 2 \\lambda \\bar{xy} \\end{pmatrix}$.\n",
    "\n",
    "With these definitions the gradient descent can be written as a `linear dicrete dynamical system`:\n",
    "\n",
    "$\\vec{s}_{t+1} = A \\vec{s}_t + B$\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (I.1) Steady state\r\n",
    "\r\n",
    "I) Assuming that the spectral radious of $A$, $\\rho(A) < 1$. I will prove that later, interestingly enough this condition gives an upper bound on the learning rate $\\lambda$ as function of the moments of the data\r\n",
    "\r\n",
    "II) This is a linear dynamical system, so if $\\rho(A)<1$ then the sequence $\\vec{s}_t$ converges to the unique steady state \r\n",
    "\r\n",
    "III) Since $\\hat{Q}(\\alpha,\\beta)$ is strictly convex in  $(\\alpha,\\beta)$ then the steady state is the global minima "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The steady state solves:\r\n",
    "\r\n",
    "$s^* \\equiv (I-A)^{-1} B $\r\n",
    "\r\n",
    "Forming $I-A$:\r\n",
    "\r\n",
    "$I - A = 2\\lambda \\begin{bmatrix}1& \\bar{x} \\\\ \\bar{x} & \\bar{x^2}\\end{bmatrix}$. Therefore the steady state can be written as\r\n",
    "\r\n",
    "$s^* = \\begin{bmatrix}1& \\bar{x} \\\\ \\bar{x} & \\bar{x^2}\\end{bmatrix}^{-1} \\begin{pmatrix} \\bar{y}\\\\ \\bar{xy} \\end{pmatrix}$\r\n",
    "\r\n",
    "`Which is the usual OLS we teach to our students` \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (I.2) Stability\r\n",
    "$A$ can be written as:\r\n",
    "\r\n",
    "$A = I - 2\\lambda \\begin{bmatrix}1& \\bar{x} \\\\ \\bar{x} & \\bar{x^2}\\end{bmatrix}$, defining\r\n",
    "\r\n",
    "$A^\\prime  \\equiv \\begin{bmatrix}1& \\bar{x} \\\\ \\bar{x} & \\bar{x^2}\\end{bmatrix}$\r\n",
    "\r\n",
    "the eigenvalues of $A$ can be written as as \r\n",
    "\r\n",
    "$1 - 2\\lambda\\times eig(A^\\prime)$. \r\n",
    "\r\n",
    "The eigenvalues o $A^\\prime$ can be written as: \r\n",
    "\r\n",
    "$\\gamma_{\\pm} = \\frac{(1+\\bar{x^2})\\pm \\sqrt{(1+\\bar{x^2})- 4 Var(x)}}{2}$\r\n",
    "\r\n",
    "Therefore choosing \r\n",
    "\r\n",
    "$\\lambda < \\frac{1}{\\min\\{Re(\\gamma_+),Re(\\gamma_-)\\} }$\r\n",
    "\r\n",
    "guarantees convergence.\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (II) Stochastic Gradient Descent "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
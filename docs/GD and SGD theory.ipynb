{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efab7cd4",
   "metadata": {},
   "source": [
    "# Convergence and Stability of Gradient Descent for Linear Regression Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32484d45",
   "metadata": {},
   "source": [
    "Consider the following problem:\n",
    "\n",
    "$\\min_{\\alpha,\\beta} \\hat{Q}(\\alpha,\\beta) \\equiv \\min_{\\alpha,\\beta} \\frac{1}{N} \\sum_{i=1}^N \\big(y_i - \\alpha - \\beta x_i\\big)^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9c66ca",
   "metadata": {},
   "source": [
    "The gradinet of $\\hat{Q}$ can be written as :\n",
    "\n",
    "$\\nabla \\hat{Q}(\\alpha,\\beta) \\equiv  \\begin{pmatrix} \\frac{\\partial \\hat{Q}(\\alpha,\\beta)}{\\partial \\alpha}\\\\ \\frac{\\partial \\hat{Q}(\\alpha,\\beta)}{\\partial \\alpha} \\end{pmatrix}=    -\\frac{2}{N} \\begin{pmatrix} \\sum_{i=1}^N  (y_i - \\beta x_i - \\alpha)\\\\  \\sum_{i=1}^N  (y_i - \\beta x_i - \\alpha)x_i \\end{pmatrix}$\n",
    "\n",
    "which can be rewritten as:\n",
    "\n",
    "$\\nabla \\hat{Q}(\\alpha,\\beta) =  -2 \\begin{pmatrix}  \\bar{y} - \\beta \\bar{x} - \\alpha\\\\  \\bar{yx} - \\beta \\bar{x^2} - \\alpha \\bar{x} \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26c4984",
   "metadata": {},
   "source": [
    "Now consider a simple gradient descent (GD) algorith that works as follows\n",
    "\n",
    "\n",
    "$\\begin{pmatrix} \\alpha_{t+1}\\\\ \\beta_{t+1} \\end{pmatrix} = \\begin{pmatrix} \\alpha_{t}\\\\ \\beta_{t} \\end{pmatrix} - \\lambda \\nabla \\hat{Q}(\\alpha_t,\\beta_t) $\n",
    "\n",
    "where $t$ is the t-$th$ step of the optimization, $\\lambda$ is the step size in the opposite direction of the gradient, and $(\\alpha_0,\\beta_0)$ is the initialization in the optimization algorithm. Exapanding the above formula leads to \n",
    "\n",
    "$\\begin{pmatrix} \\alpha_{t+1}\\\\ \\beta_{t+1} \\end{pmatrix} = \\begin{bmatrix}1 - 2\\lambda & -2 \\lambda \\bar{x}\\\\ - 2 \\lambda \\bar{x} & 1-2\\lambda \\bar{x^2}\\end{bmatrix} \\begin{pmatrix} \\alpha_{t}\\\\ \\beta_{t} \\end{pmatrix} + \\begin{pmatrix} 2 \\lambda \\bar{y}\\\\ 2 \\lambda \\bar{xy} \\end{pmatrix}$\n",
    "\n",
    "Defining $\\vec{s}_t \\equiv \\begin{pmatrix} \\alpha_{t}\\\\ \\beta_{t} \\end{pmatrix} $\n",
    "\n",
    "$A \\equiv \\begin{bmatrix}1 - 2\\lambda & -2 \\lambda \\bar{x}\\\\ - 2 \\lambda \\bar{x} & 1-2\\lambda \\bar{x^2}\\end{bmatrix} $ and  $B \\equiv \\begin{pmatrix} 2\\lambda \\bar{y}\\\\ 2 \\lambda \\bar{xy} \\end{pmatrix}$.\n",
    "\n",
    "With these definitions the gradient descent can be written as a `linear dicrete dynamical system`:\n",
    "\n",
    "$\\vec{s}_{t+1} = A \\vec{s}_t + B$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68e3f01",
   "metadata": {},
   "source": [
    "### (1) Steady state\n",
    "\n",
    "I) Assuming that the spectral radious of $A$, $\\rho(A) < 1$. I will prove that later, interestingly enough this condition gives an upper bound on the learning rate $\\lambda$ as function of the moments of the data\n",
    "\n",
    "II) This is a linear dynamical system, so if $\\rho(A)<1$ then the sequence $\\vec{s}_t$ converges to the unique steady state \n",
    "\n",
    "III) Since $\\hat{Q}(\\alpha,\\beta)$ is strictly convex in  $(\\alpha,\\beta)$ then the steady state is the global minima "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e9b00",
   "metadata": {},
   "source": [
    "The steady state solves:\n",
    "\n",
    "$s^* \\equiv (I-A)^{-1} B $\n",
    "\n",
    "Forming $I-A$:\n",
    "\n",
    "$I - A = 2\\lambda \\begin{bmatrix}1& \\bar{x} \\\\ \\bar{x} & \\bar{x^2}\\end{bmatrix}$. Therefore the steady state can be written as\n",
    "\n",
    "$s^* = \\begin{bmatrix}1& \\bar{x} \\\\ \\bar{x} & \\bar{x^2}\\end{bmatrix}^{-1} \\begin{pmatrix} \\bar{y}\\\\ \\bar{xy} \\end{pmatrix}$\n",
    "\n",
    "`Which is the usual OLS we teach to our students` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc73bb8",
   "metadata": {},
   "source": [
    "### To do:\n",
    "(1) Show under what regimes of $\\lambda$, $\\rho(A)<1$.\n",
    "\n",
    "(2) For SGD use random matrix theory and sub-sampling to show that the sequence $\\vec{s}_t$ is convergent.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23552a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
